{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Computer Vision\n",
    "\n",
    "## Project: Advanced Lane Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Declare and import dependencies on modules. Also declare global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Declare dependencies on python modules here\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Declare global variables used here\n",
    "calibration_images_path = \"camera_cal/calibration*.jpg\"\n",
    "distorted_images_path = \"distorted_images/distorted*.jpg\"\n",
    "test_images_path = \"test_images/*.jpg\"\n",
    "#Since there are 6 object points along rows and 9 object points along columns\n",
    "num_object_points = (9, 6)\n",
    "output_folder_path = \"output_images/\"\n",
    "#Initialize camera calibration coefficients\n",
    "camera_cal_coeff = {\n",
    "    \"mtx\": 0,\n",
    "    \"dist\": 0\n",
    "}\n",
    "#Declare and define source weights to be multiplied with imagexsize and imageysize respectively while taking a transform\n",
    "perspective_src_weights = np.float32([[0.1758, 0.9722], [0.4336, 0.6597], [0.5781, 0.6597], [0.8906, 0.9722]])\n",
    "#perspective_src_weights = np.float32([[0.1641, 0.9722], [0.4102, 0.6597], [0.5625, 0.6597], [0.875, 0.9722]])\n",
    "#Declare and define destination weights to be multiplied with imagexsize and imageysize respectively while taking a transform\n",
    "perspective_dst_weights = np.float32([[0.15625, 1], [0.15625, 0.1389], [0.8203, 0.1389], [0.8203, 1]])\n",
    "#Set the number of sliding windows\n",
    "nb_sliding_windows = 10\n",
    "# Set the width of the windows +/- margin\n",
    "sliding_window_margin = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 1: Compute the camera calibration matrix and distortion coefficients given a set of chessboard images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to follow:\n",
    "1. Read calibration images using glob API\n",
    "2. Create object points(3D array) for one image, this will be same for all other images\n",
    "3. Detect image points in each calibration image\n",
    "4. Add object points and image points of particular image to object points and image points array respectively\n",
    "5. Calculate camera matrix and distortion coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_images(path):\n",
    "    images = glob.glob(path)\n",
    "    return images\n",
    "images = get_images(calibration_images_path)\n",
    "# print(images)\n",
    "# plt.imshow(cv2.imread(images[0]))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_camera_calibration(draw_corners=False):\n",
    "    images = get_images(calibration_images_path)\n",
    "    obj_points = []\n",
    "    img_points = []\n",
    "    current_obj_point = np.zeros((num_object_points[1]*num_object_points[0], 3), np.float32)\n",
    "    current_obj_point[:, :2] = np.mgrid[0:9, 0:6].T.reshape(-1,2)\n",
    "    \n",
    "    for image_name in images:\n",
    "        image = cv2.imread(image_name)\n",
    "        image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        ret, corners = cv2.findChessboardCorners(image_gray, num_object_points, None)\n",
    "        \n",
    "        if ret == True:\n",
    "            current_img_point = corners\n",
    "            obj_points.append(current_obj_point)\n",
    "            img_points.append(current_img_point)\n",
    "            \n",
    "            if (draw_corners): \n",
    "                cv2.drawChessboardCorners(image, num_object_points, corners, ret)\n",
    "                plt.imshow(image)\n",
    "                plt.show()\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(obj_points, img_points, image_gray.shape[::-1], None, None)\n",
    "    return (mtx, dist)\n",
    "#print(calculate_camera_calibration(draw_corners=False))\n",
    "#camera_cal_coeff[\"mtx\"], camera_cal_coeff[\"dist\"] = calculate_camera_calibration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Apply a distortion correction to raw images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to follow:\n",
    "1. Read distorted.jpg image\n",
    "2. Apply distortion correction\n",
    "3. Save undistorted image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def undistort(img, mtx, dist, img_file_name=\"distorted\", write_output=False):\n",
    "    undist_image = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    if (write_output):\n",
    "        cv2.imwrite(output_folder_path + img_file_name.split(\".\", -1)[0] + \"-corrected.\" + img_file_name.split(\".\", -1)[1], undist_image)\n",
    "    return undist_image\n",
    "# image_path = get_images(distorted_images_path)[0]\n",
    "# distorted_image = cv2.imread(image_path)\n",
    "# plt.imshow(distorted_image)\n",
    "# plt.show()\n",
    "# distortion_corrected_image = undistort(distorted_image, \\\n",
    "#                                        camera_cal_coeff[\"mtx\"], \\\n",
    "#                                        camera_cal_coeff[\"dist\"], \\\n",
    "#                                        img_file_name=image_path.replace(\"\\\\\", \"/\").split(\"/\")[1], \\\n",
    "#                                        write_output=True)\n",
    "# plt.imshow(distortion_corrected_image)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Use color transforms, gradients, etc., to create a thresholded binary image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility method to crop image to extract the region where lane lines are most likely seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crop_image(image, ignore_mask_color=255):\n",
    "    mask = np.zeros_like(image)\n",
    "    xsize, ysize = image.shape[1], image.shape[0]\n",
    "    vertices = ((0.1565*xsize, 0.92*ysize), (xsize/2, 0.4867*ysize), (0.85*xsize, 0.92*ysize))\n",
    "    vertexArr = []\n",
    "    for vertex in vertices:\n",
    "        vertexArr.append((vertex[0], vertex[1]))\n",
    "    vertexArr = np.array([vertexArr], dtype=np.int32)\n",
    "    \n",
    "    cv2.fillPoly(mask, vertexArr, (ignore_mask_color))\n",
    "    masked_image = cv2.bitwise_and(image, mask)\n",
    "    return masked_image\n",
    "# image = cv2.cvtColor(cv2.imread(get_images(test_images_path)[0]), cv2.COLOR_BGR2GRAY)\n",
    "# masked_image = crop_image(image)\n",
    "# plt.imshow(masked_image, cmap=\"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility method to merge binary characteristics of two or more images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_binary_images(imgArr):\n",
    "    merged_image = np.zeros_like(imgArr[0])\n",
    "    for image in imgArr:\n",
    "        merged_image[(image == 1)] = 1\n",
    "    return merged_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain gradient thresholded image with respect to 'x'/'y' orient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sobel_threshold(img, sobel_kernel=3, orient=\"x\", thresh=(0, 255)):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    if orient == \"x\":\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    else:\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    abs_sobel = np.absolute(sobel)\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    binary_output[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "# image = cv2.imread(get_images(test_images_path)[7])\n",
    "# gradx = sobel_threshold(image, sobel_kernel=11, orient=\"x\", thresh=(40, 150))\n",
    "# grady = sobel_threshold(image, sobel_kernel=11, orient=\"y\", thresh=(40, 150))\n",
    "# merged = merge_binary_images([gradx, grady])\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "# plt.imshow(gradx, cmap=\"gray\")\n",
    "# plt.show()\n",
    "# plt.imshow(grady, cmap=\"gray\")\n",
    "# plt.show()\n",
    "# plt.imshow(merged, cmap=\"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain magnitude thresholded image from sobel derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def magnitude_threshold(img, sobel_kernel=3, thresh=(0, 255)):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    scaled_gradmag = np.uint8(255*gradmag/np.max(gradmag))\n",
    "    binary_output = np.zeros_like(scaled_gradmag)\n",
    "    binary_output[(scaled_gradmag >= thresh[0]) & (scaled_gradmag <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "# image = cv2.imread(get_images(test_images_path)[5])\n",
    "# mag = magnitude_threshold(image, sobel_kernel=11, thresh=(50, 150))\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "# plt.imshow(mag, cmap=\"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain magnitude thresholded image from sobel derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def direction_threshold(img, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    sobelx_abs = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel))\n",
    "    sobely_abs = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel))\n",
    "    graddir = np.arctan2(sobely_abs, sobelx_abs)\n",
    "    binary_output = np.zeros_like(graddir)\n",
    "    binary_output[(graddir >= thresh[0]) & (graddir <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "# image = cv2.imread(get_images(test_images_path)[0])\n",
    "# dir = direction_threshold(image, sobel_kernel=7, thresh=(0.8, 1.3))\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "# plt.imshow(dir, cmap=\"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtained combined image from applied gradient thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_grad_threshold(img):\n",
    "    gradx = sobel_threshold(img, sobel_kernel=11, orient=\"x\", thresh=(40, 150))\n",
    "    #grady = sobel_threshold(img, sobel_kernel=11, orient=\"y\", thresh=(40, 150))\n",
    "    mag = magnitude_threshold(img, sobel_kernel=11, thresh=(50, 150))\n",
    "    #dir = direction_threshold(img, sobel_kernel=11, thresh=(0.8, 1.3))\n",
    "    #merged_grad_image = merge_binary_images([gradx, grady, mag, dir])\n",
    "    #merged_grad_image = merge_binary_images([gradx, grady, mag])\n",
    "    merged_grad_image = merge_binary_images([gradx, mag])\n",
    "    return merged_grad_image\n",
    "# image = cv2.imread(get_images(test_images_path)[0])\n",
    "# merged_grad_image = crop_image(apply_grad_threshold(image))\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "# plt.imshow(merged_grad_image, cmap=\"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply color threshold to image. Method expects 8-bit image (Either of R,G,B,H,L,S channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_color_threshold(img):\n",
    "    r_thresh = (220, 255)\n",
    "    s_thresh = (160, 255)\n",
    "    l_thresh = (160, 255)\n",
    "    r_img = img[:,:,2]\n",
    "    r_thresholded = np.zeros_like(r_img)\n",
    "    r_thresholded[(r_img >= r_thresh[0]) & (r_img <= r_thresh[1])] = 1\n",
    "#     plt.imshow(r_thresholded, cmap=\"gray\")\n",
    "#     plt.show()\n",
    "    \n",
    "    s_img = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)[:,:,2]\n",
    "    s_thresholded = np.zeros_like(s_img)\n",
    "    s_thresholded[(s_img >= s_thresh[0]) & (s_img <= s_thresh[1])] = 1\n",
    "#     plt.imshow(s_thresholded, cmap=\"gray\")\n",
    "#     plt.show()\n",
    "    \n",
    "#     l_img = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)[:,:,2]\n",
    "#     l_thresholded = np.zeros_like(l_img)\n",
    "#     l_thresholded[(l_img >= l_thresh[0]) & (l_img <= l_thresh[1])] = 1\n",
    "#     plt.imshow(l_thresholded, cmap=\"gray\")\n",
    "#     plt.show()\n",
    "    \n",
    "    combined_img = cv2.bitwise_and(r_thresholded, s_thresholded)\n",
    "#     combined_img = np.zeros_like(s_thresholded)\n",
    "#     combined_img[(r_thresholded == 1) | (s_thresholded == 1)] = 1\n",
    "    return combined_img\n",
    "#image = cv2.imread(get_images(test_images_path)[6])\n",
    "# r_img, g_img, b_img = image[:,:,2], image[:,:,1], image[:,:,0]\n",
    "# hls_img = cv2.cvtColor(image, cv2.COLOR_BGR2HLS)\n",
    "# h_img, l_img, s_img = hls_img[:,:,0], hls_img[:,:,1], hls_img[:,:,2]\n",
    "# r_thresholded = apply_color_threshold(r_img, thresh=(175, 255))\n",
    "# s_thresholded = apply_color_threshold(s_img, thresh=(100, 255))\n",
    "# l_thresholded = apply_color_threshold(l_img, thresh=(150, 255))\n",
    "# l_and_s = np.zeros_like(l_thresholded)\n",
    "# l_and_s[(s_thresholded == 1) | (l_thresholded == 1)] = 1\n",
    "# combined_all = cv2.bitwise_and(r_thresholded, s_thresholded)\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "# # plt.imshow(r_thresholded, cmap=\"gray\")\n",
    "# # plt.show()\n",
    "# plt.imshow(s_thresholded, cmap=\"gray\")\n",
    "# plt.show()\n",
    "# # plt.imshow(l_thresholded, cmap=\"gray\")\n",
    "# # plt.show()\n",
    "# # plt.imshow(l_and_s, cmap=\"gray\")\n",
    "# # plt.show()\n",
    "# # plt.imshow(combined, cmap=\"gray\")\n",
    "# # plt.show()\n",
    "#color_thresholded = apply_color_threshold(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine color and gradient thresholds to obtain threshloded image with lane lines detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def detect_lane_lines(img):\n",
    "    grad_thresholded = apply_grad_threshold(img)\n",
    "#     plt.imshow(grad_thresholded, cmap=\"gray\")\n",
    "#     plt.show()\n",
    "    color_thresholded = apply_color_threshold(img)\n",
    "#     plt.imshow(color_thresholded, cmap=\"gray\")\n",
    "#     plt.show()\n",
    "    color_and_grad = np.zeros_like(color_thresholded)\n",
    "    color_and_grad[(grad_thresholded == 1) | (color_thresholded == 1)] = 1\n",
    "    return color_and_grad\n",
    "# image = cv2.imread(get_images(test_images_path)[6])\n",
    "# lane_image = detect_lane_lines(image)\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "# plt.imshow(lane_image, cmap=\"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Apply a perspective transform to rectify binary image (\"birds-eye view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Apply perspective transform based on selected trapezoidal points in original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_perspective_transform(img):\n",
    "    img_size = img.shape\n",
    "    src = np.concatenate(([perspective_src_weights.T[0]*img_size[1]], [perspective_src_weights.T[1]*img_size[0]]), axis=0).T\n",
    "    dst = np.concatenate(([perspective_dst_weights.T[0]*img_size[1]], [perspective_dst_weights.T[1]*img_size[0]]), axis=0).T\n",
    "    perspective_M = cv2.getPerspectiveTransform(src, dst)\n",
    "    perspective_Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    warped_img = cv2.warpPerspective(img, perspective_M, (img_size[1], img_size[0]), flags=cv2.INTER_LINEAR)\n",
    "    return (warped_img, perspective_M, perspective_Minv)\n",
    "# image = cv2.imread(get_images(test_images_path)[4])\n",
    "# #     src = np.float32([[225, 700], [555, 475], [740, 475], [1140, 700]])\n",
    "# #     dst = np.float32([[100, 720], [100, 100], [1100, 100], [1100, 720]])\n",
    "# warped_img = apply_perspective_transform(detect_lane_lines(image))[0]\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "# plt.imshow(warped_img, cmap=\"gray\")\n",
    "# plt.show()\n",
    "# histogram = np.sum(warped_img, axis=0)\n",
    "# plt.plot(histogram)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Detect lane pixels and fit to find the lane boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sliding window search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sliding_window_search(binary_warped_img):\n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(binary_warped_img[binary_warped_img.shape[0]/2:,:], axis=0)\n",
    "    \n",
    "    # Create an output image to draw on and  visualize the result\n",
    "    output_img = np.dstack((binary_warped_img, binary_warped_img, binary_warped_img))*255\n",
    "    #output_img = np.copy(img)\n",
    "    \n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    midpoint = np.int(histogram.shape[0]/2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "    \n",
    "    # Set height of windows\n",
    "    window_height = np.int(binary_warped_img.shape[0]/nb_sliding_windows)\n",
    "    \n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped_img.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    \n",
    "    # Current positions to be updated for each window. Initialized to left and right base\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "    \n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 2000\n",
    "    \n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "    \n",
    "    for window in range(nb_sliding_windows):\n",
    "        good_left_inds = []\n",
    "        good_right_inds = []\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = binary_warped_img.shape[0] - (window+1)*window_height\n",
    "        win_y_high = binary_warped_img.shape[0] - window*window_height\n",
    "        win_xleft_low = leftx_current - sliding_window_margin\n",
    "        win_xleft_high = leftx_current + sliding_window_margin\n",
    "        win_xright_low = rightx_current - sliding_window_margin\n",
    "        win_xright_high = rightx_current + sliding_window_margin\n",
    "        # Draw the windows on the visualization image\n",
    "        cv2.rectangle(output_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),(0,255,0), 2) \n",
    "        cv2.rectangle(output_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),(0,255,0), 2) \n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:        \n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "    # Concatenate the arrays of indices\n",
    "    left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds] \n",
    "\n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    \n",
    "    ploty = np.linspace(0, binary_warped_img.shape[0]-1, binary_warped_img.shape[0] )\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "\n",
    "    output_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "    output_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.plot(left_fitx, ploty, color='yellow')\n",
    "#     plt.plot(right_fitx, ploty, color='yellow')\n",
    "#     plt.xlim(0, 1280)\n",
    "#     plt.ylim(720, 0)\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "    return (output_img, lefty, righty, leftx, rightx, left_fitx, right_fitx)\n",
    "# image = cv2.imread(get_images(test_images_path)[4])\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "# warped_img = apply_perspective_transform(detect_lane_lines(image))[0]\n",
    "# output_img = sliding_window_search(warped_img)[0]\n",
    "# plt.imshow(output_img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Determine the curvature of the lane and vehicle position with respect to center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine radius from left and right line plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_radius(img, lefty, righty, leftx, rightx):\n",
    "    ploty = np.linspace(0, img.shape[0]-1, img.shape[0])\n",
    "    y_eval = np.max(ploty)\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "\n",
    "    # Fit new polynomials to x,y in world space\n",
    "    left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "    right_fit_cr = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "    # Calculate the new radii of curvature\n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    avg_radius = (left_curverad + right_curverad)/2\n",
    "    return (avg_radius)\n",
    "# image = cv2.imread(get_images(test_images_path)[6])\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "# warped_img = apply_perspective_transform(detect_lane_lines(image))\n",
    "# output_img, lefty, righty, leftx, rightx = sliding_window_search(image, warped_img)\n",
    "# left_curverad, right_curverad = extract_radius(output_img, lefty, righty, leftx, rightx)\n",
    "# print(left_curverad, right_curverad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Warp the detected lane boundaries back onto the original image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract lane detected image and warp boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def warp_lanes(img, warped, perspective_Minv, left_fitx, right_fitx):\n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(warped).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "    \n",
    "    ploty = np.linspace(0, warped.shape[0]-1, warped.shape[0] )\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "#     plt.imshow(color_warp)\n",
    "#     plt.show()\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, perspective_Minv, (img.shape[1], img.shape[0])) \n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(img, 1, newwarp, 0.3, 0)\n",
    "    return result\n",
    "# image = cv2.imread(get_images(test_images_path)[7])\n",
    "# camera_cal_coeff[\"mtx\"], camera_cal_coeff[\"dist\"] = calculate_camera_calibration()\n",
    "# distortion_corrected_image = undistort(image, \\\n",
    "#                                        camera_cal_coeff[\"mtx\"], \\\n",
    "#                                        camera_cal_coeff[\"dist\"]) \n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "# # plt.imshow(distortion_corrected_image)\n",
    "# # plt.show()\n",
    "# warped_img, p_M, p_Minv = apply_perspective_transform(detect_lane_lines(distortion_corrected_image))\n",
    "# output_img, lefty, righty, leftx, rightx, left_fitx, right_fitx = sliding_window_search(image, warped_img)\n",
    "# result = warp_lanes(distortion_corrected_image, warped_img, p_Minv, left_fitx, right_fitx)\n",
    "# plt.imshow(result)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write text to the image for displaying radius of curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_text(img, text, text_bottom_left_corner, fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=1, color=(255, 255, 255)):\n",
    "    annotated_img = np.copy(img)\n",
    "    cv2.putText(annotated_img, text, text_bottom_left_corner, fontFace, fontScale, color)\n",
    "    return annotated_img\n",
    "# image = cv2.imread(get_images(test_images_path)[0])\n",
    "# annotated_img = write_text(image, \"Sample text\", (100, 50))\n",
    "# cv2.imwrite(output_folder_path + \"test.jpg\", annotated_img)\n",
    "# plt.imshow(annotated_img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Lane Finding algorithm on video clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def laneDetectionInVideo (sourceFilePath, outputFilePath, write_output=False):\n",
    "    #originalVideoClip = VideoFileClip(sourceFilePath).subclip(0, 0.5)\n",
    "    originalVideoClip = VideoFileClip(sourceFilePath)\n",
    "    laneDetectedClip = originalVideoClip.fl_image(advanced_lane_detection)\n",
    "#     if (write_output):\n",
    "#         %time laneDetectedClip.write_videofile(outputFilePath, audio=False)\n",
    "#     else:\n",
    "#         %time\n",
    "    %time laneDetectedClip.write_videofile(outputFilePath, audio=False)\n",
    "#laneDetectionInVideo(\"challenge.mp4\", \"final-challenge-lane-detected.mp4\")\n",
    "#laneDetectionInVideo(\"solidYellowLeft.mp4\", \"final-yellow-lane-detected.mp4\")\n",
    "#laneDetectionInVideo(\"solidWhiteRight.mp4\", \"final-white-lane-detected-plain.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Image, Detect Lanes, Draw lane markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def advanced_lane_detection(img):\n",
    "    #plt.imshow(img)\n",
    "    #plt.show()\n",
    "    \n",
    "    #Undistort image\n",
    "    distortion_corrected = undistort(img, \\\n",
    "                                camera_cal_coeff[\"mtx\"], \\\n",
    "                                camera_cal_coeff[\"dist\"])\n",
    "    #plt.imshow(distortion_corrected)\n",
    "    #plt.show()\n",
    "    \n",
    "    #Detect lane lines after applying gradient and color threshold.\n",
    "    lane_detected_img = detect_lane_lines(distortion_corrected)\n",
    "    #plt.imshow(lane_detected_img, cmap=\"gray\")\n",
    "    #plt.show()\n",
    "    \n",
    "    #Apply perspective transform to get bird's eye view of lane lines\n",
    "    warped_img, perspective_M, perspective_Minv = apply_perspective_transform(lane_detected_img)\n",
    "    #plt.imshow(warped_img, cmap=\"gray\")\n",
    "    #plt.show()\n",
    "    \n",
    "    #Apply sliding window search algorithm to fit lane lines to a second order polynomial\n",
    "    window_detected_img, lefty, righty, leftx, rightx, left_fitx, right_fitx = sliding_window_search(warped_img)\n",
    "    \n",
    "    #Detect lane curvature and radius\n",
    "    radius = extract_radius(window_detected_img, lefty, righty, leftx, rightx)\n",
    "    #print(\"Left curve radius is: \" + str(left_curverad) + \"|||| Right curve radius is: \" + str(right_curverad))\n",
    "    #print(radius)\n",
    "    \n",
    "    #Warp lane lines and lane detected area back to original image\n",
    "    result = warp_lanes(img, warped_img, perspective_Minv, left_fitx, right_fitx)\n",
    "    #plt.imshow(result)\n",
    "    #plt.show()\n",
    "    \n",
    "    annotated_img = write_text(result, \"Radius of curvature is: \" + str(radius), (100, 50))\n",
    "#     plt.imshow(annotated_img)\n",
    "#     plt.show()\n",
    "    \n",
    "    return annotated_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video challenge-video-solution.mp4\n",
      "[MoviePy] Writing video challenge-video-solution.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                  | 0/485 [00:00<?, ?it/s]\n",
      "  0%|                                          | 1/485 [00:00<02:07,  3.81it/s]\n",
      "  0%|▏                                         | 2/485 [00:00<02:04,  3.88it/s]\n",
      "  1%|▎                                         | 3/485 [00:00<02:03,  3.91it/s]\n",
      "  1%|▎                                         | 4/485 [00:01<02:05,  3.83it/s]\n",
      "  1%|▍                                         | 5/485 [00:01<02:06,  3.80it/s]\n",
      "  1%|▌                                         | 6/485 [00:01<02:17,  3.49it/s]\n",
      "  1%|▌                                         | 7/485 [00:01<02:13,  3.59it/s]\n",
      "  2%|▋                                         | 8/485 [00:02<02:11,  3.63it/s]\n",
      "  2%|▊                                         | 9/485 [00:02<02:13,  3.56it/s]\n",
      "  2%|▊                                        | 10/485 [00:02<02:14,  3.53it/s]\n",
      "  2%|▉                                        | 11/485 [00:03<02:09,  3.66it/s]\n",
      "  2%|█                                        | 12/485 [00:03<02:05,  3.78it/s]\n",
      "  3%|█                                        | 13/485 [00:03<02:05,  3.75it/s]\n",
      "  3%|█▏                                       | 14/485 [00:03<02:07,  3.70it/s]\n",
      "  3%|█▎                                       | 15/485 [00:04<02:10,  3.61it/s]\n",
      "  3%|█▎                                       | 16/485 [00:04<02:08,  3.64it/s]\n",
      "  4%|█▍                                       | 17/485 [00:04<02:09,  3.63it/s]\n",
      "  4%|█▌                                       | 18/485 [00:04<02:11,  3.55it/s]\n",
      "  4%|█▌                                       | 19/485 [00:05<02:13,  3.49it/s]\n",
      "  4%|█▋                                       | 20/485 [00:05<02:10,  3.56it/s]\n",
      "  4%|█▊                                       | 21/485 [00:05<02:10,  3.57it/s]\n",
      "  5%|█▊                                       | 22/485 [00:06<02:03,  3.76it/s]\n",
      "  5%|█▉                                       | 23/485 [00:06<02:08,  3.60it/s]\n",
      "  5%|██                                       | 24/485 [00:06<02:08,  3.60it/s]\n",
      "  5%|██                                       | 25/485 [00:06<02:05,  3.67it/s]\n",
      "  5%|██▏                                      | 26/485 [00:07<02:13,  3.44it/s]\n",
      "  6%|██▎                                      | 27/485 [00:07<02:12,  3.45it/s]\n",
      "  6%|██▎                                      | 28/485 [00:07<02:23,  3.19it/s]\n",
      "  6%|██▍                                      | 29/485 [00:08<02:19,  3.26it/s]\n",
      "  6%|██▌                                      | 30/485 [00:09<04:57,  1.53it/s]\n",
      "  6%|██▌                                      | 31/485 [00:11<07:55,  1.05s/it]\n",
      "  7%|██▋                                      | 32/485 [00:12<08:24,  1.11s/it]\n",
      "  7%|██▊                                      | 33/485 [00:13<06:35,  1.14it/s]\n",
      "  7%|██▊                                      | 34/485 [00:13<05:11,  1.45it/s]\n",
      "  7%|██▉                                      | 35/485 [00:13<04:19,  1.74it/s]\n",
      "  7%|███                                      | 36/485 [00:14<03:44,  2.00it/s]\n",
      "  8%|███▏                                     | 37/485 [00:14<03:21,  2.23it/s]\n",
      "  8%|███▏                                     | 38/485 [00:14<02:55,  2.55it/s]\n",
      "  8%|███▎                                     | 39/485 [00:14<02:36,  2.84it/s]\n",
      "  8%|███▍                                     | 40/485 [00:15<02:23,  3.10it/s]\n",
      "  8%|███▍                                     | 41/485 [00:15<02:17,  3.24it/s]\n",
      "  9%|███▌                                     | 42/485 [00:15<02:17,  3.23it/s]\n",
      "  9%|███▋                                     | 43/485 [00:16<02:19,  3.17it/s]\n",
      "  9%|███▋                                     | 44/485 [00:16<02:33,  2.88it/s]\n",
      "  9%|███▊                                     | 45/485 [00:16<02:24,  3.05it/s]\n",
      "  9%|███▉                                     | 46/485 [00:17<02:19,  3.16it/s]\n",
      " 10%|███▉                                     | 47/485 [00:17<02:22,  3.07it/s]\n",
      " 10%|████                                     | 48/485 [00:17<02:20,  3.11it/s]\n",
      " 10%|████▏                                    | 49/485 [00:18<02:20,  3.10it/s]\n",
      " 10%|████▏                                    | 50/485 [00:18<02:23,  3.03it/s]\n",
      " 11%|████▎                                    | 51/485 [00:18<02:26,  2.97it/s]\n",
      " 11%|████▍                                    | 52/485 [00:19<02:30,  2.88it/s]\n",
      " 11%|████▍                                    | 53/485 [00:19<02:27,  2.92it/s]\n",
      " 11%|████▌                                    | 54/485 [00:19<02:29,  2.89it/s]\n",
      " 11%|████▋                                    | 55/485 [00:20<02:27,  2.91it/s]\n",
      " 12%|████▋                                    | 56/485 [00:21<03:50,  1.86it/s]\n",
      " 12%|████▊                                    | 57/485 [00:23<07:23,  1.04s/it]\n",
      " 12%|████▉                                    | 58/485 [00:26<11:07,  1.56s/it]\n",
      " 12%|████▉                                    | 59/485 [00:26<08:39,  1.22s/it]\n",
      " 12%|█████                                    | 60/485 [00:26<06:57,  1.02it/s]\n",
      " 13%|█████▏                                   | 61/485 [00:27<05:40,  1.24it/s]\n",
      " 13%|█████▏                                   | 62/485 [00:27<04:37,  1.53it/s]\n",
      " 13%|█████▎                                   | 63/485 [00:27<03:54,  1.80it/s]\n",
      " 13%|█████▍                                   | 64/485 [00:28<03:32,  1.98it/s]\n",
      " 13%|█████▍                                   | 65/485 [00:28<03:18,  2.12it/s]\n",
      " 14%|█████▌                                   | 66/485 [00:29<02:51,  2.44it/s]\n",
      " 14%|█████▋                                   | 67/485 [00:29<02:40,  2.60it/s]\n",
      " 14%|█████▋                                   | 68/485 [00:29<02:41,  2.59it/s]\n",
      " 14%|█████▊                                   | 69/485 [00:30<02:30,  2.76it/s]\n",
      " 14%|█████▉                                   | 70/485 [00:30<02:27,  2.82it/s]\n",
      " 15%|██████                                   | 71/485 [00:30<02:29,  2.77it/s]\n",
      " 15%|██████                                   | 72/485 [00:31<02:26,  2.82it/s]\n",
      " 15%|██████▏                                  | 73/485 [00:31<02:21,  2.92it/s]\n",
      " 15%|██████▎                                  | 74/485 [00:31<02:18,  2.97it/s]\n",
      " 15%|██████▎                                  | 75/485 [00:32<02:14,  3.05it/s]\n",
      " 16%|██████▍                                  | 76/485 [00:32<02:10,  3.12it/s]\n",
      " 16%|██████▌                                  | 77/485 [00:32<02:03,  3.30it/s]\n",
      " 16%|██████▌                                  | 78/485 [00:32<02:00,  3.39it/s]\n",
      " 16%|██████▋                                  | 79/485 [00:33<01:58,  3.43it/s]\n",
      " 16%|██████▊                                  | 80/485 [00:33<01:54,  3.54it/s]\n",
      " 17%|██████▊                                  | 81/485 [00:33<01:51,  3.63it/s]\n",
      " 17%|██████▉                                  | 82/485 [00:34<03:02,  2.20it/s]\n",
      " 17%|███████                                  | 83/485 [00:35<04:13,  1.58it/s]\n",
      " 17%|███████                                  | 84/485 [00:36<05:15,  1.27it/s]\n",
      " 18%|███████▏                                 | 85/485 [00:38<06:09,  1.08it/s]\n",
      " 18%|███████▎                                 | 86/485 [00:38<05:14,  1.27it/s]\n",
      " 18%|███████▎                                 | 87/485 [00:38<04:16,  1.55it/s]\n",
      " 18%|███████▍                                 | 88/485 [00:39<03:34,  1.85it/s]\n",
      " 18%|███████▌                                 | 89/485 [00:39<03:28,  1.90it/s]\n",
      " 19%|███████▌                                 | 90/485 [00:40<03:18,  1.99it/s]\n",
      " 19%|███████▋                                 | 91/485 [00:40<02:54,  2.26it/s]\n",
      " 19%|███████▊                                 | 92/485 [00:40<02:40,  2.45it/s]\n",
      " 19%|███████▊                                 | 93/485 [00:40<02:22,  2.75it/s]\n",
      " 19%|███████▉                                 | 94/485 [00:41<02:09,  3.01it/s]\n",
      " 20%|████████                                 | 95/485 [00:41<02:03,  3.16it/s]\n",
      " 20%|████████                                 | 96/485 [00:41<01:57,  3.30it/s]\n",
      " 20%|████████▏                                | 97/485 [00:41<01:52,  3.46it/s]\n",
      " 20%|████████▎                                | 98/485 [00:42<01:51,  3.48it/s]\n",
      " 20%|████████▎                                | 99/485 [00:42<01:56,  3.30it/s]\n",
      " 21%|████████▏                               | 100/485 [00:42<01:54,  3.36it/s]\n",
      " 21%|████████▎                               | 101/485 [00:43<01:52,  3.42it/s]\n",
      " 21%|████████▍                               | 102/485 [00:43<01:49,  3.50it/s]\n",
      " 21%|████████▍                               | 103/485 [00:43<01:50,  3.45it/s]\n",
      " 21%|████████▌                               | 104/485 [00:44<01:49,  3.48it/s]\n",
      " 22%|████████▋                               | 105/485 [00:44<01:49,  3.48it/s]\n",
      " 22%|████████▋                               | 106/485 [00:44<01:42,  3.69it/s]\n",
      " 22%|████████▊                               | 107/485 [00:44<01:42,  3.69it/s]\n",
      " 22%|████████▉                               | 108/485 [00:45<01:46,  3.54it/s]\n",
      " 22%|████████▉                               | 109/485 [00:45<01:48,  3.45it/s]\n",
      " 23%|█████████                               | 110/485 [00:45<01:46,  3.53it/s]\n",
      " 23%|█████████▏                              | 111/485 [00:46<03:06,  2.00it/s]\n",
      " 23%|█████████▏                              | 112/485 [00:47<04:29,  1.38it/s]\n",
      " 23%|█████████▎                              | 113/485 [00:49<05:48,  1.07it/s]\n",
      " 24%|█████████▍                              | 114/485 [00:50<05:52,  1.05it/s]\n",
      " 24%|█████████▍                              | 115/485 [00:50<04:35,  1.35it/s]\n",
      " 24%|█████████▌                              | 116/485 [00:50<03:40,  1.67it/s]\n",
      " 24%|█████████▋                              | 117/485 [00:51<03:06,  1.98it/s]\n",
      " 24%|█████████▋                              | 118/485 [00:51<02:45,  2.22it/s]\n",
      " 25%|█████████▊                              | 119/485 [00:51<02:33,  2.39it/s]\n",
      " 25%|█████████▉                              | 120/485 [00:52<02:18,  2.63it/s]\n",
      " 25%|█████████▉                              | 121/485 [00:52<02:15,  2.69it/s]\n",
      " 25%|██████████                              | 122/485 [00:52<02:06,  2.88it/s]\n",
      " 25%|██████████▏                             | 123/485 [00:53<02:03,  2.94it/s]\n",
      " 26%|██████████▏                             | 124/485 [00:53<02:04,  2.90it/s]\n",
      " 26%|██████████▎                             | 125/485 [00:53<01:56,  3.09it/s]\n",
      " 26%|██████████▍                             | 126/485 [00:54<01:51,  3.22it/s]\n",
      " 26%|██████████▍                             | 127/485 [00:54<01:50,  3.25it/s]\n",
      " 26%|██████████▌                             | 128/485 [00:54<01:53,  3.15it/s]\n",
      " 27%|██████████▋                             | 129/485 [00:54<01:53,  3.13it/s]\n",
      " 27%|██████████▋                             | 130/485 [00:55<01:53,  3.13it/s]\n",
      " 27%|██████████▊                             | 131/485 [00:55<01:50,  3.19it/s]\n",
      " 27%|██████████▉                             | 132/485 [00:55<01:44,  3.37it/s]\n",
      " 27%|██████████▉                             | 133/485 [00:56<01:38,  3.56it/s]\n",
      " 28%|███████████                             | 134/485 [00:56<01:34,  3.70it/s]\n",
      " 28%|███████████▏                            | 135/485 [00:56<01:33,  3.72it/s]\n",
      " 28%|███████████▏                            | 136/485 [00:56<01:34,  3.70it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected non-empty vector for x",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-b79961229fb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcamera_cal_coeff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"mtx\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcamera_cal_coeff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dist\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_camera_calibration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlaneDetectionInVideo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_src\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_dst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test_video/challenge_video.mp4\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"challenge-video-solution.mp4\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[1;31m# image = cv2.imread(get_images(test_images_path)[5])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;31m# plt.imshow(image)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-b79961229fb4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(video_src, video_dst, write_output)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[1;31m#Calculate camera calibration matrix and cache mtx and dist values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcamera_cal_coeff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"mtx\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcamera_cal_coeff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dist\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_camera_calibration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlaneDetectionInVideo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_src\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_dst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test_video/challenge_video.mp4\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"challenge-video-solution.mp4\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;31m# image = cv2.imread(get_images(test_images_path)[5])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-df10888e68c6>\u001b[0m in \u001b[0;36mlaneDetectionInVideo\u001b[0;34m(sourceFilePath, outputFilePath, write_output)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;31m#     else:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;31m#         %time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time laneDetectedClip.write_videofile(outputFilePath, audio=False)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[1;31m#laneDetectionInVideo(\"challenge.mp4\", \"final-challenge-lane-detected.mp4\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;31m#laneDetectionInVideo(\"solidYellowLeft.mp4\", \"final-yellow-lane-detected.mp4\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ssohon\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ssohon\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2080\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ssohon\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ssohon\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'eval'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-173>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ssohon\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\moviepy\\decorators.py\u001b[0m in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Attribute 'duration' not set\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-172>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ssohon\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\moviepy\\decorators.py\u001b[0m in \u001b[0;36muse_clip_fps_by_default\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m    135\u001b[0m              for (k,v) in k.items()}\n\u001b[1;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<decorator-gen-171>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ssohon\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\moviepy\\decorators.py\u001b[0m in \u001b[0;36mconvert_masks_to_RGB\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismask\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mclip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_RGB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ssohon\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\moviepy\\video\\VideoClip.py\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params)\u001b[0m\n\u001b[1;32m    337\u001b[0m                            \u001b[0maudiofile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maudiofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                            \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthreads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                            ffmpeg_params=ffmpeg_params)\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mremove_temp\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmake_audio\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ssohon\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\moviepy\\video\\io\\ffmpeg_writer.py\u001b[0m in \u001b[0;36mffmpeg_write_video\u001b[0;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     for t,frame in clip.iter_frames(progress_bar=True, with_times=True,\n\u001b[0;32m--> 204\u001b[0;31m                                     fps=fps, dtype=\"uint8\"):\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwithmask\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ssohon\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    829\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[1;31m# Update and print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ssohon\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\moviepy\\Clip.py\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m                 \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-136>\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ssohon\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\moviepy\\decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(f, *a, **kw)\u001b[0m\n\u001b[1;32m     87\u001b[0m         new_kw = {k: fun(v) if k in varnames else v\n\u001b[1;32m     88\u001b[0m                  for (k,v) in kw.items()}\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ssohon\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\moviepy\\Clip.py\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ssohon\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\moviepy\\Clip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[1;31m#mf = copy(self.make_frame)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mnewclip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_make_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ssohon\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\moviepy\\video\\VideoClip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(gf, t)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mby\u001b[0m \u001b[0manother\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mimage_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \"\"\"\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mgf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimage_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[1;31m# --------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-10c400c386ed>\u001b[0m in \u001b[0;36madvanced_lane_detection\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[1;31m#Apply sliding window search algorithm to fit lane lines to a second order polynomial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mwindow_detected_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlefty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrighty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleftx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrightx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_fitx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_fitx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msliding_window_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwarped_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[1;31m#Detect lane curvature and radius\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-52c2acb4899e>\u001b[0m in \u001b[0;36msliding_window_search\u001b[0;34m(binary_warped_img)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[1;31m# Fit a second order polynomial to each\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mleft_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolyfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlefty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleftx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mright_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolyfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrighty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrightx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ssohon\\AppData\\Local\\Continuum\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\numpy\\lib\\polynomial.py\u001b[0m in \u001b[0;36mpolyfit\u001b[0;34m(x, y, deg, rcond, full, w, cov)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"expected 1D vector for x\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"expected non-empty vector for x\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"expected 1D or 2D array for y\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected non-empty vector for x"
     ]
    }
   ],
   "source": [
    "def main(video_src, video_dst, write_output=False):\n",
    "    #Calculate camera calibration matrix and cache mtx and dist values\n",
    "    camera_cal_coeff[\"mtx\"], camera_cal_coeff[\"dist\"] = calculate_camera_calibration()\n",
    "    laneDetectionInVideo(video_src, video_dst)\n",
    "main(\"test_video/challenge_video.mp4\", \"challenge-video-solution.mp4\", write_output=False)\n",
    "# image = cv2.imread(get_images(test_images_path)[5])\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "# lane_detected_img = detect_lane_lines(image)\n",
    "# plt.imshow(lane_detected_img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
