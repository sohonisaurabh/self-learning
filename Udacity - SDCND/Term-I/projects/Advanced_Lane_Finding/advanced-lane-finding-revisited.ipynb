{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Computer Vision\n",
    "\n",
    "## Project: Advanced Lane Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Declare and import dependencies on modules. Also declare global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Declare dependencies on python modules here\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Declare global variables used here\n",
    "calibration_images_path = \"camera_cal/calibration*.jpg\"\n",
    "distorted_images_path = \"distorted_images/distorted*.jpg\"\n",
    "test_images_path = \"test_images/*.jpg\"\n",
    "#Since there are 6 object points along rows and 9 object points along columns\n",
    "num_object_points = (9, 6)\n",
    "output_folder_path = \"output_images/\"\n",
    "#Initialize camera calibration coefficients\n",
    "camera_cal_coeff = {\n",
    "    \"mtx\": 0,\n",
    "    \"dist\": 0\n",
    "}\n",
    "#Declare and define source weights to be multiplied with imagexsize and imageysize respectively while taking a transform\n",
    "perspective_src_weights = np.float32([[0.1758, 0.9722], [0.4336, 0.6597], [0.5781, 0.6597], [0.8906, 0.9722]])\n",
    "#perspective_src_weights = np.float32([[0.1641, 0.9722], [0.4102, 0.6597], [0.5625, 0.6597], [0.875, 0.9722]])\n",
    "#Declare and define destination weights to be multiplied with imagexsize and imageysize respectively while taking a transform\n",
    "perspective_dst_weights = np.float32([[0.15625, 1], [0.15625, 0.1389], [0.8203, 0.1389], [0.8203, 1]])\n",
    "#Set the number of sliding windows\n",
    "nb_sliding_windows = 10\n",
    "# Set the width of the windows +/- margin\n",
    "sliding_window_margin = 50\n",
    "# Define conversions in x and y from pixels space to meters\n",
    "ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "#Cached Lane lines list\n",
    "global cached_lane_line\n",
    "cached_lane_line = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 1: Compute the camera calibration matrix and distortion coefficients given a set of chessboard images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to follow:\n",
    "1. Read calibration images using glob API\n",
    "2. Create object points(3D array) for one image, this will be same for all other images\n",
    "3. Detect image points in each calibration image\n",
    "4. Add object points and image points of particular image to object points and image points array respectively\n",
    "5. Calculate camera matrix and distortion coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility method to get path of images along with image name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_image_paths(path):\n",
    "    images = glob.glob(path)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to find chessboard corners and draw on calibration image if specified. This method also calculates camera calibration mtx and dist coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_camera_calibration(draw_corners=False):\n",
    "    images = get_image_paths(calibration_images_path)\n",
    "    obj_points = []\n",
    "    img_points = []\n",
    "    current_obj_point = np.zeros((num_object_points[1]*num_object_points[0], 3), np.float32)\n",
    "    current_obj_point[:, :2] = np.mgrid[0:9, 0:6].T.reshape(-1,2)\n",
    "    \n",
    "    for image_name in images:\n",
    "        image = cv2.imread(image_name)\n",
    "        image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        ret, corners = cv2.findChessboardCorners(image_gray, num_object_points, None)\n",
    "        \n",
    "        if ret == True:\n",
    "            current_img_point = corners\n",
    "            obj_points.append(current_obj_point)\n",
    "            img_points.append(current_img_point)\n",
    "            \n",
    "            if (draw_corners): \n",
    "                cv2.drawChessboardCorners(image, num_object_points, corners, ret)\n",
    "                plt.imshow(image)\n",
    "                plt.show()\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(obj_points, img_points, image_gray.shape[::-1], None, None)\n",
    "    return (mtx, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Apply a distortion correction to raw images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to follow:\n",
    "1. Read distorted.jpg image\n",
    "2. Apply distortion correction\n",
    "3. Save undistorted image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to undistort (correct distortion in a distorted image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def undistort(img, mtx, dist, img_file_name=\"distorted\", write_output=False):\n",
    "    undist_image = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    if (write_output):\n",
    "        cv2.imwrite(output_folder_path + img_file_name.split(\".\", -1)[0] + \"-corrected.\" + img_file_name.split(\".\", -1)[1], undist_image)\n",
    "    return undist_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Use color transforms, gradients, etc., to create a thresholded binary image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility method to merge binary characteristics of two or more images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_binary_images(imgArr):\n",
    "    merged_image = np.zeros_like(imgArr[0])\n",
    "    for image in imgArr:\n",
    "        merged_image[(image == 1)] = 1\n",
    "    return merged_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to obtain gradient thresholded image with respect to specified 'x' or 'y' orient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sobel_threshold(img, sobel_kernel=3, orient=\"x\", thresh=(0, 255)):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    if orient == \"x\":\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    else:\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    abs_sobel = np.absolute(sobel)\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    binary_output[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to obtain magnitude thresholded image from sobel derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def magnitude_threshold(img, sobel_kernel=3, thresh=(0, 255)):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    scaled_gradmag = np.uint8(255*gradmag/np.max(gradmag))\n",
    "    binary_output = np.zeros_like(scaled_gradmag)\n",
    "    binary_output[(scaled_gradmag >= thresh[0]) & (scaled_gradmag <= thresh[1])] = 1\n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to obtain direction thresholded image from sobel derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def direction_threshold(img, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    sobelx_abs = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel))\n",
    "    sobely_abs = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel))\n",
    "    graddir = np.arctan2(sobely_abs, sobelx_abs)\n",
    "    binary_output = np.zeros_like(graddir)\n",
    "    binary_output[(graddir >= thresh[0]) & (graddir <= thresh[1])] = 1\n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to obtain combined image from applied gradient thresholds. In this project, gradient in 'x' direction and magnitude component of overall gradient in used for detecting lane lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_grad_threshold(img):\n",
    "    gradx = sobel_threshold(img, sobel_kernel=11, orient=\"x\", thresh=(40, 150))\n",
    "    mag = magnitude_threshold(img, sobel_kernel=11, thresh=(50, 150))\n",
    "    merged_grad_image = merge_binary_images([gradx, mag])\n",
    "    return merged_grad_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply color threshold to image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_color_threshold(img):\n",
    "    #Declare 'R' channel threshold in RGB image\n",
    "    r_thresh = (220, 255)\n",
    "    #Declare 'S' channel threshold in HLS image\n",
    "    s_thresh = (160, 200)\n",
    "    \n",
    "    #Extract 'R' channel in BGR image and apply threshold\n",
    "    r_img = img[:,:,2]\n",
    "    r_thresholded = np.zeros_like(r_img)\n",
    "    r_thresholded[(r_img >= r_thresh[0]) & (r_img <= r_thresh[1])] = 1\n",
    "\n",
    "    #Extract 'S' channel from HLS image and apply threshold\n",
    "    s_img = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)[:,:,2]\n",
    "    s_thresholded = np.zeros_like(s_img)\n",
    "    s_thresholded[(s_img >= s_thresh[0]) & (s_img <= s_thresh[1])] = 1\n",
    "\n",
    "    #Use characteristics derived from 'R' thresholded image with 'S' thresholded image\n",
    "    combined_img = np.zeros_like(s_thresholded)\n",
    "    combined_img[(r_thresholded == 1) | (s_thresholded == 1)] = 1\n",
    "    return combined_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to combine color and gradient thresholds to obtain threshloded image with lane lines detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def detect_lane_lines(img):\n",
    "    grad_thresholded = apply_grad_threshold(img)\n",
    "    color_thresholded = apply_color_threshold(img)\n",
    "    \n",
    "    #Use characteristics derived from combined gradient image with combined color thresholded image\n",
    "    color_and_grad = np.zeros_like(color_thresholded)\n",
    "    color_and_grad[(grad_thresholded == 1) | (color_thresholded == 1)] = 1\n",
    "    return color_and_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Apply a perspective transform to rectify binary image (\"birds-eye view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Method to apply perspective transform based on selected trapezoidal points in original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_perspective_transform(img):\n",
    "    img_size = img.shape\n",
    "    \n",
    "    #Calculate actual pixel points by multiplying weights with x and y dimensions of image\n",
    "    src = np.concatenate(([perspective_src_weights.T[0]*img_size[1]], [perspective_src_weights.T[1]*img_size[0]]), axis=0).T\n",
    "    dst = np.concatenate(([perspective_dst_weights.T[0]*img_size[1]], [perspective_dst_weights.T[1]*img_size[0]]), axis=0).T\n",
    "    \n",
    "    #Calculate perspective matrix\n",
    "    perspective_M = cv2.getPerspectiveTransform(src, dst)\n",
    "    \n",
    "    #Calculate inverse perspective matrix\n",
    "    perspective_Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    \n",
    "    #Warp image to get a bird's eye view\n",
    "    warped_img = cv2.warpPerspective(img, perspective_M, (img_size[1], img_size[0]), flags=cv2.INTER_LINEAR)\n",
    "    return (warped_img, perspective_M, perspective_Minv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Detect lane pixels and fit to find the lane boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sliding window search. This method is used for searching lane lines in warped image and fitting them to a polynomial. Most of the code used in this method is re-used from the one shared in 'Project: Advanced Lane Detection' class and is attributed Udacity. Few changes made to the algorithm are as follows:\n",
    "\n",
    "    1. Histogram is taken for bottom 66% of the image and then starting point of lane line detection is derived.\n",
    "    2. Minimum number of pixels found to recenter window is updated to 2000\n",
    "    3. Margin of sliding window is set to 50 pixels right and left of the midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sliding_window_search(binary_warped_img):\n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(binary_warped_img[binary_warped_img.shape[0]/3:,:], axis=0)\n",
    "    \n",
    "    # Create an output image to draw on and  visualize the result\n",
    "    output_img = np.dstack((binary_warped_img, binary_warped_img, binary_warped_img))*255\n",
    "    #output_img = np.copy(img)\n",
    "    \n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    midpoint = np.int(histogram.shape[0]/2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "    \n",
    "    # Set height of windows\n",
    "    window_height = np.int(binary_warped_img.shape[0]/nb_sliding_windows)\n",
    "    \n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped_img.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    \n",
    "    # Current positions to be updated for each window. Initialized to left and right base\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "    \n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 2000\n",
    "    \n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "    \n",
    "    for window in range(nb_sliding_windows):\n",
    "        good_left_inds = []\n",
    "        good_right_inds = []\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = binary_warped_img.shape[0] - (window+1)*window_height\n",
    "        win_y_high = binary_warped_img.shape[0] - window*window_height\n",
    "        win_xleft_low = leftx_current - sliding_window_margin\n",
    "        win_xleft_high = leftx_current + sliding_window_margin\n",
    "        win_xright_low = rightx_current - sliding_window_margin\n",
    "        win_xright_high = rightx_current + sliding_window_margin\n",
    "        # Draw the windows on the visualization image\n",
    "        cv2.rectangle(output_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),(0,255,0), 2) \n",
    "        cv2.rectangle(output_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),(0,255,0), 2) \n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:        \n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "    # Concatenate the arrays of indices\n",
    "    left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds] \n",
    "\n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    \n",
    "    ploty = np.linspace(0, binary_warped_img.shape[0]-1, binary_warped_img.shape[0] )\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "\n",
    "    output_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "    output_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "\n",
    "    return (output_img, lefty, righty, leftx, rightx, left_fitx, right_fitx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Determine the curvature of the lane and vehicle position with respect to center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to determine radius from fit polynomials for left and right lane lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_radius(img, lefty, righty, leftx, rightx):\n",
    "    ploty = np.linspace(0, img.shape[0]-1, img.shape[0])\n",
    "    y_eval = np.max(ploty)/2\n",
    "\n",
    "    # Fit new polynomials to x,y in world space\n",
    "    left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "    right_fit_cr = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "    \n",
    "    # Calculate the new radii of curvature\n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    \n",
    "    if ((right_curverad < 2000 or left_curverad < 2000) and \\\n",
    "        abs(right_curverad - left_curverad) > 1500):\n",
    "        #Measurement of one or both lane lines is higher than offset. Take average of half of the measurement\n",
    "        avg_radius = (left_curverad*0.5 + right_curverad*0.5)/2\n",
    "    else:\n",
    "        avg_radius = (left_curverad + right_curverad)/2\n",
    "    \n",
    "    #Return average, left and right lane line radius\n",
    "    return (avg_radius, left_curverad, right_curverad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Warp the detected lane boundaries back onto the original image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mehod to extract lane detected image and warp boundaries to original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def warp_lanes(img, warped, perspective_Minv, left_fitx, right_fitx):\n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(warped).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "    \n",
    "    ploty = np.linspace(0, warped.shape[0]-1, warped.shape[0])\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0, 255, 0))\n",
    "    \n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, perspective_Minv, (img.shape[1], img.shape[0]))\n",
    "\n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(img, 1, newwarp, 0.3, 0)\n",
    "    return (result, newwarp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to write text to the image for displaying radius of curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_text(img, text, text_bottom_left_corner, fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=1, color=(255, 255, 255)):\n",
    "    annotated_img = np.copy(img)\n",
    "    cv2.putText(annotated_img, text, text_bottom_left_corner, fontFace, fontScale, color)\n",
    "    return annotated_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to read image frames from video, apply lane finding algorithm and write lane detected image to output video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def laneDetectionInVideo (sourceFilePath, outputFilePath):\n",
    "    originalVideoClip = VideoFileClip(sourceFilePath)\n",
    "    laneDetectedClip = originalVideoClip.fl_image(advanced_lane_detection)\n",
    "    %time laneDetectedClip.write_videofile(outputFilePath, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Image, Detect Lanes, Draw lane markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility method to check for erroneous measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_for_erroneous_measurement(current_lane):\n",
    "    global cached_lane_line\n",
    "\n",
    "    if (cached_lane_line == None):\n",
    "        cached_lane_line = current_lane\n",
    "    else:\n",
    "        #Perform polygon matching to check if the shape of polygon bounded by lanes is admissible\n",
    "        match_ret = cv2.matchShapes(cached_lane_line.lane_polygon, current_lane.lane_polygon ,1, 0.0)\n",
    "        #Dissimilarities are detected\n",
    "        if (match_ret > 0.045):\n",
    "            current_lane = cached_lane_line\n",
    "        #If polygon matched is similar, check for avg_radius values to be in of 500m in comparison with last measurement\n",
    "        else:\n",
    "            if (abs(current_lane.avg_radius - cached_lane_line.avg_radius) > 1000):\n",
    "                current_lane.avg_radius = cached_lane_line.avg_radius\n",
    "            #There is no error in current measurement\n",
    "            else:\n",
    "                cached_lane_line = current_lane\n",
    "                cached_lane_line.avg_radius = (cached_lane_line.avg_radius + current_lane.avg_radius)/2\n",
    "    return current_lane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to run every processing step from the pipeline and draw lane markers on original image frame from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def advanced_lane_detection(img, check_for_error_results=True):\n",
    "    \n",
    "    #Create instance of Lane class\n",
    "    current_lane = Lane()\n",
    "    \n",
    "    #Undistort image\n",
    "    distortion_corrected = undistort(img, \\\n",
    "                                camera_cal_coeff[\"mtx\"], \\\n",
    "                                camera_cal_coeff[\"dist\"])\n",
    "    \n",
    "    #Detect lane lines after applying gradient and color threshold.\n",
    "    lane_detected_img = detect_lane_lines(distortion_corrected)\n",
    "    \n",
    "    #Apply perspective transform to get bird's eye view of lane lines\n",
    "    warped_img, perspective_M, perspective_Minv = apply_perspective_transform(lane_detected_img)\n",
    "    \n",
    "    #Apply sliding window search algorithm to fit lane lines to a second order polynomial\n",
    "    window_detected_img, lefty, righty, leftx, rightx, left_fitx, right_fitx = sliding_window_search(warped_img)\n",
    "    \n",
    "    #Detect lane curvature and radius\n",
    "    radius, left_curverad, right_curverad = extract_radius(window_detected_img, lefty, righty, leftx, rightx)\n",
    "    \n",
    "    #Warp lane lines and lane detected area back to original image\n",
    "    result, lane_polygon = warp_lanes(img, warped_img, perspective_Minv, left_fitx, right_fitx)\n",
    "    \n",
    "    #Set detected lane properties\n",
    "    current_lane.set_avg_radius(radius)\n",
    "    current_lane.set_lane_polygon(cv2.cvtColor(lane_polygon, cv2.COLOR_RGB2GRAY))\n",
    "    \n",
    "    if (check_for_error_results):\n",
    "        #Check detection for erroneous measurements\n",
    "        current_lane = check_for_erroneous_measurement(current_lane)\n",
    "    \n",
    "    #Calculate centroid of lane detected region to determine the position of vehicle w.r.t to lane lane area\n",
    "    lane_polygon = cv2.cvtColor(lane_polygon, cv2.COLOR_BGR2GRAY)\n",
    "    moments = cv2.moments(lane_polygon)\n",
    "    cX = int(moments[\"m10\"] / moments[\"m00\"])\n",
    "    image_x_center = img.shape[1]/2\n",
    "    \n",
    "    #Write radius value to image\n",
    "    annotated_img = write_text(result, \"Radius of curvature = \" + str(current_lane.avg_radius) + \"(m)\", (100, 50))\n",
    "\n",
    "    #Write vehicle position value to image\n",
    "    if (cX > image_x_center):\n",
    "        annotated_img = write_text(annotated_img, \"Vehicle is: \" + str(round((cX - image_x_center)*xm_per_pix, 4)) + \"m left of center\", (100, 90))\n",
    "    else:\n",
    "        annotated_img = write_text(annotated_img, \"Vehicle is: \" + str(round((image_x_center - cX)*xm_per_pix, 4)) + \"m right of center\", (100, 90))\n",
    "    \n",
    "    return annotated_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching of results to smooth the lane search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare Lane class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Lane():\n",
    "    def __init__(self):\n",
    "        self.lane_polygon = None\n",
    "        self.avg_radius = None\n",
    "        \n",
    "    def set_lane_polygon(self, polygon_img):\n",
    "        self.lane_polygon = np.copy(polygon_img)\n",
    "        \n",
    "    def set_avg_radius(self, avg_radius):\n",
    "        self.avg_radius = avg_radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(apply_on_video=False, images_path=\"test_images/*.jpg\", images_output_path=\"output_images/\", video_src=\"\", video_dst=\"\"):\n",
    "    #Calculate camera calibration matrix and cache mtx and dist values\n",
    "    camera_cal_coeff[\"mtx\"], camera_cal_coeff[\"dist\"] = calculate_camera_calibration()\n",
    "    if (apply_on_video):\n",
    "        laneDetectionInVideo(video_src, video_dst)\n",
    "    else:\n",
    "        image_paths = get_image_paths(images_path)\n",
    "        for image_path in image_paths:\n",
    "            image = cv2.imread(image_path)\n",
    "            annotated_image = advanced_lane_detection(image, check_for_error_results=False)\n",
    "            plt.imshow(annotated_image)\n",
    "            plt.show()\n",
    "            cv2.imwrite(images_output_path + image_path.replace(\"\\\\\", \"/\").split(\"/\")[-1].split(\".\", -1)[0] + \"_lane_detected.\" + image_path.split(\".\", -1)[1], annotated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Run lane detection pipeline on test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#main(images_path=test_images_path, images_output_path=output_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run lane detection pipeline of video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video project-solution-final.mp4\n",
      "[MoviePy] Writing video project-solution-final.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████▉| 1260/1261 [07:49<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: project-solution-final.mp4 \n",
      "\n",
      "Wall time: 7min 52s\n"
     ]
    }
   ],
   "source": [
    "#main(apply_on_video=True, video_src=\"test_video/project_video.mp4\", video_dst=\"output_video/project_video_lane_detected.mp4\")\n",
    "main(apply_on_video=True, video_src=\"test_video/project_video.mp4\", video_dst=\"project-solution-final.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
